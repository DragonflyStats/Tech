\documentclass[12pt, a4paper]{report}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\bibliographystyle{chicago}
\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\begin{document}
\author{Kevin O'Brien}
\title{MA4125}

\tableofcontents \setcounter{tocdepth}{2}
%-------------------------------------------------







%------------------------------------------------------------------------%
\chapter{Linear Models}
\section{Multiple Linear Regression}
\section{Variable Selection Procedures}
   

\section{Data analytics}
Data analytics (DA) is the science of examining raw data with the purpose of drawing conclusions about that information. Data analytics is used in many industries to allow companies and organization to make better business decisions and in the sciences to verify or disprove existing models or theories. Data analytics is distinguished from data mining by the scope, purpose and focus of the analysis. Data miners sort through huge data sets using sophisticated software to identify undiscovered patterns and establish hidden relationships. Data analytics focuses on inference, the process of deriving a conclusion based solely on what is already known by the researcher.




\section{Customer Relations}

\subsection{Customer Valuation}
In customer relationship management (CRM), customer valuation is a scoring process used to help a company determine which customers the company should target in order to maximize profit. Customer valuation requires that the company evaluate past data to learn which customers purchased recently, which customers purchased frequently, and which customers spent the most money, in hopes that the company can forecast future purchase potential and make sure time and resources are spent only on its best customers.


To understand how customer valuation works, let's imagine there is a company that manufactures skateboards called CoolSkate. CoolSkate's sales are made primarily through Internet and print catalog sales. Through surveys and questionnaires on their Web site, CoolSkate has accumulated quite a bit of data about the buying habits, preferences, and age range of their customers. With this data, CoolSkate will devise a customer valuation scoring system that awards points based on total purchasing dollars, repeat purchases, and customer loyalty. CoolSkate can then use the information gained from the customer valuation scores to predict repeat-purchase probability as well as the probability of attrition, and target their promotions to customers who are likely to make new purchases.

Customer valuation is based upon the 80/20 rule in marketing, whereby a company spends the majority of its time working with its best customers. There are many software applications on the market to help companies determine a point system relevant to their products or services and combine aggregated data to determine customer valuation.



\subsubsection{Applications of real-time analytics}
In CRM (customer relations management), real-time analytics can provide up-to-the-minute information about an enterprise's customers and present it so that better and quicker business decisions can be made -- perhaps even within the time span of a customer interaction. Real-time analytics can support instant refreshes to corporate dashboards to reflect business changes throughout the day. In a data warehouse context, real-time analytics supports unpredictable, ad hoc queries against large data sets. Another application is in scientific analysis such as the tracking of a hurricane's path, intensity, and wind field, with the intent of predicting these parameters hours or days in advance.

The adjective real-time refers to a level of computer responsiveness that a user senses as immediate or nearly immediate, or that enables a computer to keep up with some external process (for example, to present visualizations of Web site activity as it constantly changes).

\subsection{Churn Modelling }





\subsection{Descriptive modeling}
Descriptive modeling is a mathematical process that describes real-world events and the relationships between factors responsible for them. The process is used by consumer-driven organizations to help them target their marketing and advertising efforts.

In descriptive modeling, customer groups are clustered according to demographics, purchasing behavior, expressed interests and other descriptive factors. Statistics can identify where the customer groups share similarities and where they differ. The most active customers get special attention because they offer the greatest return on investment.

The main aspects of descriptive modeling include:

\begin{itemize}
\item Customer segmentation: Partitions a customer base into groups with various impacts on marketing and service.
\item Value-based segmentation: Identifies and quantifies the value of a customer to the organization.
\item Behavior-based segmentation: Analyzes customer product usage and purchasing patterns.
\item Needs-based segmentation: Identifies ways to capitalize on motives that drive customer behavior.
\end{itemize}
Descriptive modeling can help an organization to understand its customers, but predictive modeling is necessary to facilitate the desired outcomes. Both descriptive and predictive modeling constitute key elements of data mining and Web mining.

\subsection{Big Data}
Big data (also spelled Big Data) is a general term used to describe the voluminous amount of unstructured and semi-structured data a company creates, data that would take too much time and cost too much money to load into a relational database for analysis. Although Big data doesn't refer to any specific quantity, the term is often used when speaking about \textbf{\emph{petabytes}} and \textbf{\emph{exabytes}} of data.


A primary goal for looking at big data is to discover repeatable business patterns. Its generally accepted that unstructured data, most of it located in text files, accounts for at least $80\%$ of an organizations data. If left unmanaged, the sheer volume of unstructured data thats generated each year within an enterprise can be costly in terms of storage. Unmanaged data can also pose a liability if information cannot be located in the event of a compliance audit or lawsuit.

Big data analytics is often associated with cloud computing because the analysis of large data sets in real-time requires a framework like \textbf{\emph{MapReduce}} to distribute the work among tens, hundreds or even thousands of computers.

\section{Memory Issues}
\subsection{Yottabytes}







%-------------------------------------------------------------%
\chapter{Contemporary Issues in Computing}
\section{Cloud computing}
Cloud computing is a general term for anything that involves delivering hosted services over the Internet. These services are broadly divided into three categories: Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS) and Software-as-a-Service (SaaS). The name cloud computing was inspired by the cloud symbol that's often used to represent the Internet in flowcharts and diagrams.


A cloud service has three distinct characteristics that differentiate it from traditional hosting.
\begin{itemize} 
\item It is sold on demand, typically by the minute or the hour 
\item It is elastic -- a user can have as much or as little of a service as they want at any given time
\item The service is fully managed by the provider (the consumer needs nothing but a personal computer and Internet access). 
\end{itemize}

Significant innovations in \textbf{\emph{virtualization}} and \textbf{\emph{distributed computing}}, as well as improved access to high-speed Internet and a weak economy, have accelerated interest in cloud computing.

A cloud can be private or public. A public cloud sells services to anyone on the Internet. (Currently, Amazon Web Services is the largest public cloud provider.) A private cloud is a proprietary network or a data center that supplies hosted services to a limited number of people. When a service provider uses public cloud resources to create their private cloud, the result is called a virtual private cloud. 

Private or public, the goal of cloud computing is to provide easy, scalable access to computing resources and IT services.


\textbf{\emph{Infrastructure-as-a-Service}} like Amazon Web Services provides virtual server instanceAPI) to start, stop, access and configure their virtual servers and storage. In the enterprise, cloud computing allows a company to pay for only as much capacity as is needed, and bring more online as soon as required. Because this pay-for-what-you-use model resembles the way electricity, fuel and water are consumed, it's sometimes referred to as utility computing.


\textbf{\emph{Platform-as-a-service}} in the cloud is defined as a set of software and product development tools hosted on the provider's infrastructure. Developers create applications on the provider's platform over the Internet. PaaS providers may use APIs, website portals or gateway software installed on the customer's computer. Force.com, (an outgrowth of Salesforce.com) and GoogleApps are examples of PaaS. Developers need to know that currently, there are not standards for interoperability or data portability in the cloud. Some providers will not allow software created by their customers to be moved off the provider's platform.


In the \textbf{\emph{software-as-a-service}} cloud model, the vendor supplies the hardware infrastructure, the software product and interacts with the user through a front-end portal. \textbf{\emph{SaaS}} is a very broad market. Services can be anything from Web-based email to inventory control and database processing. Because the service provider hosts both the application and the data, the end user is free to use the service from anywhere.





\chapter{Databases}
\subsection{Relational Database}
A relational database is a collection of data items organized as a set of formally-described tables from which data can be accessed or reassembled in many different ways without having to reorganize the database tables. The relational database was invented by E. F. Codd at IBM in 1970.

The standard user and application program interface to a relational database is the structured query language (SQL). SQL statements are used both for interactive queries for information from a relational database and for gathering data for reports.

In addition to being relatively easy to create and access, a relational database has the important advantage of being easy to extend. After the original database creation, a new data category can be added without requiring that all existing applications be modified.

A relational database is a set of tables containing data fitted into predefined categories. Each table (which is sometimes called a relation) contains one or more data categories in columns. Each row contains a unique instance of data for the categories defined by the columns. For example, a typical business order entry database would include a table that described a customer with columns for name, address, phone number, and so forth. Another table would describe an order: product, customer, date, sales price, and so forth. A user of the database could obtain a view of the database that fitted the user's needs. For example, a branch office manager might like a view or report on all customers that had bought products after a certain date. A financial services manager in the same company could, from the same tables, obtain a report on accounts that needed to be paid.

When creating a relational database, you can define the domain of possible values in a data column and further constraints that may apply to that data value. For example, a domain of possible customers could allow up to ten possible customer names but be constrained in one table to allowing only three of these customer names to be specifiable.

The definition of a relational database results in a table of metadata or formal descriptions of the tables, columns, domains, and constraints.


\subsection{denormalization}

In a relational database, denormalization is an approach to speeding up read performance (data retrieval) in which the administrator selectively adds back specific instances of redundant data after the data structure has been normalized. A denormalized database should not be confused with a database that has never been normalized.

During normalization, the database designer stores different but related types of data in separate logical tables called relations. When a query combines data from multiple tables into a single result table, it is called a join. Multiple joins in the same query can have a negative impact on performance. Introducing denormalization and adding back a small number of redundancies can be a useful for cutting down on the number of joins.

After data has been duplicated, the database designer must take into account how multiple instances of the data will be maintained. One way to denormalize a database is to allow the database management system (DBMS) to store redundant information on disk. This has the added benefit of ensuring the consistency of redundant copies. Another approach is to denormalize the actual logical data design, but this can quickly lead to inconsistent data. 

Rules called constraints can be used to specify how redundant copies of information are synchronized, but they increase the complexity of the database design and also run the risk of impacting write performance.




%------------------------------------------------%
%\newpage
%\addcontentsline{toc}{section}{Bibliography}
%\bibliography{MA4125bib}
\end{document} 

Contents
CRAN TaskView:  High Performance Computing	1
Apache Hadoop	1
bigmemory	1
MapReduce	2
Parallel Computing	2
Revolution Analytics	4



Revolution Analytics
Revolution R Enterprise Breaks Through R’s Memory Barrier for Big Data Analysis
Revolution Analytics has taken the popular R language to unprecedented new levels of capacity and performance for statistical analysis of very large data sets. Using the built-in RevoScaleR package, R users can process, visualize and model terabyte-class data sets in a fraction of the time of legacy products – without requiring expensive or specialized hardware.
Big-Data statistical algorithms make use of all available computing resources for high-performance analysis, without data size limitations. Revolution R Enterprise includes distributed, multi-threaded implementations of the following algorithms, with more planned for future updates: •Summary Statistics
 
•Crosstabulations
•Correlation and Covariance
•Linear Regression
•Binomial Logistic Regression
•Principal Components Analysis
•Generalized Linear Modeling
•K-means clustering 
•Predictions
